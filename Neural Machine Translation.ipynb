{"cells":[{"cell_type":"markdown","metadata":{},"source":["Importing All Required Dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import re\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TimeDistributed, Attention, Concatenate, Input\n","from tensorflow.keras.models import Model\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping\n","from nltk.translate.bleu_score import corpus_bleu"]},{"cell_type":"markdown","metadata":{},"source":["Data Loading and Cleaning"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T18:17:03.377450Z","iopub.status.busy":"2024-05-25T18:17:03.376999Z","iopub.status.idle":"2024-05-25T18:17:18.466657Z","shell.execute_reply":"2024-05-25T18:17:18.465814Z","shell.execute_reply.started":"2024-05-25T18:17:03.377390Z"},"trusted":true},"outputs":[],"source":["def load_and_clean_data(file_path):\n","    # Read the data from the file\n","    df = pd.read_csv(file_path, sep=\"\\t\", usecols=[0, 1], names=[\"English\", \"French\"])\n","\n","    # Define a function to clean text\n","    def clean_text(text):\n","        # Remove non-alphabetic characters and convert to lowercase\n","        text = re.sub(r\"[^a-zA-Z\\s]\", '', text).lower()\n","        # Remove extra whitespaces\n","        text = re.sub(r'\\s+', ' ', text).strip()\n","        return text\n","\n","    # Apply the cleaning function to both English and French columns\n","    df['English'] = df['English'].apply(clean_text)\n","    df['French'] = df['French'].apply(clean_text)\n","\n","    return df\n","\n","# Load and clean the data\n","df = load_and_clean_data(\"/kaggle/input/fra.txt\") #Insert path of the File that contains English text translated to French\n"]},{"cell_type":"markdown","metadata":{},"source":["Data Splitting and Tokenization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def split_and_tokenize_data(df, max_len=30):\n","    # Split the dataset into features (X) and labels (y)\n","    X = df['English']\n","    y = df['French']\n","\n","    # Split the data into training, validation, and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n","\n","    # Initialize tokenizers for English and French languages\n","    tokenizer_English = Tokenizer(oov_token='<UNK>')\n","    tokenizer_French = Tokenizer(oov_token='<UNK>')\n","\n","    # Fit tokenizers on training data\n","    tokenizer_English.fit_on_texts(X_train)\n","    tokenizer_French.fit_on_texts(y_train)\n","\n","    # Define a function to convert text to padded sequences\n","    def text_to_padded_sequences(tokenizer, texts, max_len):\n","        sequences = tokenizer.texts_to_sequences(texts)\n","        return pad_sequences(sequences, maxlen=max_len, padding='post')\n","\n","    # Convert training, validation, and testing data to padded sequences\n","    padded_sequences_train_English = text_to_padded_sequences(tokenizer_English, X_train, max_len)\n","    padded_sequences_train_French = text_to_padded_sequences(tokenizer_French, y_train, max_len).reshape((-1, max_len, 1))\n","    padded_sequences_val_English = text_to_padded_sequences(tokenizer_English, X_val, max_len)\n","    padded_sequences_val_French = text_to_padded_sequences(tokenizer_French, y_val, max_len).reshape((-1, max_len, 1))\n","    padded_sequences_test_English = text_to_padded_sequences(tokenizer_English, X_test, max_len)\n","    padded_sequences_test_French = text_to_padded_sequences(tokenizer_French, y_test, max_len).reshape((-1, max_len, 1))\n","\n","    # Return the padded sequences along with the tokenizers\n","    return (padded_sequences_train_English, padded_sequences_train_French,\n","            padded_sequences_val_English, padded_sequences_val_French,\n","            padded_sequences_test_English, padded_sequences_test_French,\n","            tokenizer_English, tokenizer_French)\n","\n","# Call the function to split and tokenize the data\n","(padded_sequences_train_English, padded_sequences_train_French,\n"," padded_sequences_val_English, padded_sequences_val_French,\n"," padded_sequences_test_English, padded_sequences_test_French,\n"," tokenizer_English, tokenizer_French) = split_and_tokenize_data(df)\n"]},{"cell_type":"markdown","metadata":{},"source":["Embedding Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T18:17:18.468500Z","iopub.status.busy":"2024-05-25T18:17:18.468236Z","iopub.status.idle":"2024-05-25T18:17:24.950977Z","shell.execute_reply":"2024-05-25T18:17:24.949830Z","shell.execute_reply.started":"2024-05-25T18:17:18.468478Z"},"trusted":true},"outputs":[],"source":["def load_embeddings(glove_path, embedding_dim):\n","    # Initialize an empty dictionary to store word embeddings\n","    embedding_index = {}\n","    # Open the GloVe embedding file\n","    with open(glove_path, encoding='utf-8') as f:\n","        # Iterate through each line in the file\n","        for line in f:\n","            # Split the line by whitespace to separate the word and its embedding vector\n","            values = line.split()\n","            # Extract the word (first element) from the line\n","            word = values[0]\n","            # Extract the embedding vector (remaining elements) and convert it to a numpy array of float32 dtype\n","            coefs = np.asarray(values[1:], dtype='float32')\n","            # Store the word and its embedding vector in the dictionary\n","            embedding_index[word] = coefs\n","    # Return the dictionary containing word embeddings\n","    return embedding_index\n","\n","def create_embedding_matrix(embedding_index, tokenizer, embedding_dim):\n","    # Determine the size of the vocabulary based on the tokenizer's word index\n","    vocab_size = len(tokenizer.word_index) + 1\n","    # Initialize an empty matrix to store the embedding vectors for each word in the vocabulary\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","    # Iterate through each word in the tokenizer's word index\n","    for word, i in tokenizer.word_index.items():\n","        # Retrieve the embedding vector for the word from the embedding index\n","        embedding_vector = embedding_index.get(word)\n","        # If the word exists in the embedding index, update the corresponding row in the embedding matrix with its embedding vector\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    # Return the embedding matrix\n","    return embedding_matrix\n","\n","# Load GloVe embeddings for English\n","embedding_index_English = load_embeddings(\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\", 50) #Insert Glove Dataset for English\n","# Create embedding matrix for English\n","embedding_matrix_English = create_embedding_matrix(embedding_index_English, tokenizer_English, 50)\n","\n","# Load GloVe embeddings for French\n","embedding_index_French = load_embeddings(\"/kaggle/input/cc.fr.300.vec\", 300) #Insert Glove Dataset for French\n","# Create embedding matrix for French\n","embedding_matrix_French = create_embedding_matrix(embedding_index_French, tokenizer_French, 300)\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Building"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def build_model(vocab_size_English, vocab_size_French, embedding_matrix_English, embedding_matrix_French, max_len):\n","    # Define embedding layers for English and French using pre-trained embedding matrices\n","    embedding_layer_English = Embedding(input_dim=vocab_size_English, output_dim=50,\n","                                        embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_English),\n","                                        trainable=False)\n","    embedding_layer_French = Embedding(input_dim=vocab_size_French, output_dim=300,\n","                                       embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix_French),\n","                                       trainable=False)\n","\n","    # Encoder\n","    encoder_input = Input(shape=(max_len,))\n","    encoder_embedding = embedding_layer_English(encoder_input)\n","    encoder_bi_lstm = Bidirectional(LSTM(units=128, return_sequences=True))(encoder_embedding)\n","    encoder_output, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(units=128, return_sequences=True, return_state=True))(encoder_bi_lstm)\n","    state_h = Concatenate()([forward_h, backward_h])\n","    state_c = Concatenate()([forward_c, backward_c])\n","\n","    # Decoder\n","    decoder_input = Input(shape=(max_len,))\n","    decoder_embedding = embedding_layer_French(decoder_input)\n","    decoder_lstm = LSTM(units=256, return_sequences=True)(decoder_embedding, initial_state=[state_h, state_c])\n","    attention = Attention()([decoder_lstm, encoder_output])\n","    decoder_concat = Concatenate()([decoder_lstm, attention])\n","    decoder_output = TimeDistributed(Dense(vocab_size_French, activation='softmax'))(decoder_concat)\n","\n","    # Create the model\n","    model = Model([encoder_input, decoder_input], decoder_output)\n","    return model\n","\n","# Build the model\n","model = build_model(len(tokenizer_English.word_index) + 1, len(tokenizer_French.word_index) + 1,\n","                    embedding_matrix_English, embedding_matrix_French, max_len=30)\n","\n","# Compile the model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T18:17:24.996182Z","iopub.status.busy":"2024-05-25T18:17:24.995843Z","iopub.status.idle":"2024-05-25T18:17:25.305227Z","shell.execute_reply":"2024-05-25T18:17:25.304313Z","shell.execute_reply.started":"2024-05-25T18:17:24.996156Z"},"trusted":true},"outputs":[],"source":["def train_model(model, padded_sequences_train_English, padded_sequences_train_French,\n","                padded_sequences_val_English, padded_sequences_val_French, epochs=20, batch_size=64):\n","    # Define EarlyStopping callback to prevent overfitting\n","    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, min_delta=0.001)\n","    \n","    # Train the model\n","    history = model.fit(\n","        [padded_sequences_train_English, padded_sequences_train_French],\n","        padded_sequences_train_French,\n","        validation_data=([padded_sequences_val_English, padded_sequences_val_French], padded_sequences_val_French),\n","        verbose=1, batch_size=batch_size, epochs=epochs,\n","        callbacks=[early_stopping]\n","    )\n","    \n","    return history\n","\n","# Train the model\n","history = train_model(model, padded_sequences_train_English, padded_sequences_train_French,\n","                      padded_sequences_val_English, padded_sequences_val_French)\n"]},{"cell_type":"markdown","metadata":{},"source":["Model Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-25T18:17:25.306697Z","iopub.status.busy":"2024-05-25T18:17:25.306378Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, padded_sequences_test_English, padded_sequences_test_French):\n","    # Evaluate the model on the testing data\n","    test_loss, test_accuracy = model.evaluate([padded_sequences_test_English, padded_sequences_test_French], padded_sequences_test_French)\n","    \n","    # Print the test loss and accuracy\n","    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n","    \n","    # Return the test loss and accuracy\n","    return test_loss, test_accuracy\n","\n","# Call the evaluate_model function to evaluate the model\n","test_loss, test_accuracy = evaluate_model(model, padded_sequences_test_English, padded_sequences_test_French)\n"]},{"cell_type":"markdown","metadata":{},"source":["Translation and BLEU Score Calculation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def translate_sentence(model, tokenizer_English, tokenizer_French, sentence, max_len):\n","    # Convert the input sentence to a sequence of tokens using the English tokenizer\n","    sequence = tokenizer_English.texts_to_sequences([sentence])\n","    # Pad the sequence to ensure uniform length\n","    padded_sequence = pad_sequences(sequence, maxlen=max_len, padding='post')\n","    # Predict the translation using the trained model\n","    prediction = model.predict([padded_sequence, padded_sequence])\n","    # Convert the predicted sequence of indices to a sentence using the French tokenizer\n","    predicted_sequence = [np.argmax(vector) for vector in prediction[0]]\n","    predicted_sentence = tokenizer_French.sequences_to_texts([predicted_sequence])\n","    return predicted_sentence[0]\n","\n","def calculate_bleu_score(model, X_test, y_test, tokenizer_English, tokenizer_French, max_len=30):\n","    # Initialize a list to store the predicted translations\n","    predicted_translations = []\n","    # Iterate over each English sentence in the test set\n","    for english_sentence in X_test:\n","        # Translate the English sentence to French\n","        translated_sentence = translate_sentence(model, tokenizer_English, tokenizer_French, english_sentence, max_len)\n","        # Split the translated sentence into tokens and append it to the list of predicted translations\n","        predicted_translations.append(translated_sentence.split())\n","\n","    # Split the true translations into tokens\n","    true_translations = [sentence.split() for sentence in y_test]\n","    # Compute the BLEU score using the NLTK library\n","    bleu_score = corpus_bleu([[ref] for ref in true_translations], predicted_translations)\n","    # Print the computed BLEU score\n","    print(\"BLEU Score:\", bleu_score)\n","    # Return the BLEU score\n","    return bleu_score\n"]},{"cell_type":"markdown","metadata":{},"source":["Main Code"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def main():\n","    # Load and clean data\n","    file_path = \"/kaggle/input/dataset-1/fra.txt\"  # Path to the dataset file\n","    df = load_and_clean_data(file_path)  # Load and clean the dataset\n","\n","    # Split and tokenize data\n","    (padded_sequences_train_English, padded_sequences_train_French,\n","     padded_sequences_val_English, padded_sequences_val_French,\n","     padded_sequences_test_English, padded_sequences_test_French,\n","     tokenizer_English, tokenizer_French) = split_and_tokenize_data(df, max_len=30)\n","\n","    # Load embeddings\n","    glove_path_English = \"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\"  # Path to GloVe embeddings for English\n","    glove_path_French = \"/kaggle/input/cc.fr.300.vec\"  # Path to GloVe embeddings for French\n","    embedding_index_English = load_embeddings(glove_path_English, 50)  # Load GloVe embeddings for English\n","    embedding_index_French = load_embeddings(glove_path_French, 300)  # Load GloVe embeddings for French\n","    embedding_matrix_English = create_embedding_matrix(embedding_index_English, tokenizer_English, 50)  # Create embedding matrix for English\n","    embedding_matrix_French = create_embedding_matrix(embedding_index_French, tokenizer_French, 300)  # Create embedding matrix for French\n","\n","    # Build and compile the model\n","    model = build_model(len(tokenizer_English.word_index) + 1, len(tokenizer_French.word_index) + 1,\n","                        embedding_matrix_English, embedding_matrix_French, max_len=30)\n","    model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n","    \n","    # Train the model\n","    history = train_model(model, padded_sequences_train_English, padded_sequences_train_French,\n","                          padded_sequences_val_English, padded_sequences_val_French, epochs=20, batch_size=64)\n","\n","    # Save the trained model\n","    model_save_path = \"translation_model.h5\"  # Define the path where the model will be saved\n","    model.save(model_save_path)\n","    print(f\"Model saved to {model_save_path}\")\n","\n","    # Evaluate the model on the testing data\n","    test_loss, test_accuracy = evaluate_model(model, padded_sequences_test_English, padded_sequences_test_French)\n","    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n","\n","    # Calculate BLEU score\n","    bleu_score = calculate_bleu_score(model, df['English'].iloc[:100], df['French'].iloc[:100], tokenizer_English, tokenizer_French, max_len=30)\n","    print(\"BLEU Score:\", bleu_score)\n","\n","# Call the main function to execute the entire pipeline\n","if __name__ == \"__main__\":\n","    main()\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":5082121,"sourceId":8513096,"sourceType":"datasetVersion"},{"datasetId":8542,"sourceId":11957,"sourceType":"datasetVersion"}],"dockerImageVersionId":30698,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
